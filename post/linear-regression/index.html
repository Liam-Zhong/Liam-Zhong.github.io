<!doctype html>







<html
  class="not-ready lg:text-base"
  style="--bg:#faf8f1"
  lang="en"
  dir="ltr"
><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>线性回归 - Chlzhong</title>

  
  <meta name="theme-color" />

  <meta name="description" content="线性回归理解起来很简单，当参数样本只有一个特征与一个标签时就是高中学最小二乘法的那个模型，两个特征及以上时则是大学概率论上学的参数估计，只不过估计参数时选用了梯度下降法。
值得注意的是损失函数的选择，为什么是平方误差函数而不是差的绝对值呢？这里其实是因为我们假定偏差的分布符合高斯分布，其对应的概率密度函数就是平方和形式，而且差的绝对值（拉普拉斯分布）所对应的极大似然函数没法方便寻优。
而说到梯度下降，在工程实现方面其实也不难，具体见后文代码，主要谈谈背后的数学。
0.梯度是什么
对于一个多元函数，梯度是该函数所有偏导数的向量，指向函数增长最快的方向，大小表示增长的速率。
假设有一个函数 $f(x_1,x_2,…,x_n)$，梯度可以表示为一个向量：
$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$
这里，$\frac{\partial f}{\partial x_i}$ 是函数 $f$ 对每个变量 $x_i$ 的偏导数。
1. 梯度下降与泰勒展开的关系
先解释下为什么要有梯度下降法：其实最简单的二维凸函数是抛物线 $f(x)=x^2$，很容易通过解方程 $f&rsquo;(x)=0$ 求出最小值在 $x=0$ 处；只是有一些凸函数这样解方程太麻烦，便用梯度下降法来找最值。
最简单情况 ($f(x)=x^2$) 下，若将给定点 $x_0$ 加上 -$\eta\nabla f(x_0)$，就相当于一个逐渐靠近最低点的物理过程。比如取 $x_0$ = 10，$\eta = 0.2$ ，迭代 10 次左右就是差不多靠近了最低点 $x=0$

初始值 x=10, f(x)=100
第 1 次迭代: x=6.0, f(x)=36.0
第 2 次迭代: x=3.6, f(x)=12.96
第 3 次迭代: x=2.16, f(x)=4.67
第 4 次迭代: x=1.296, f(x)=1.68
第 5 次迭代: x=0.7776, f(x)=0.60
第 6 次迭代: x=0.46656, f(x)=0.22
第 7 次迭代: x=0.27994, f(x)=0.078
第 8 次迭代: x=0.16796, f(x)=0.028
第 9 次迭代: x=0.10078, f(x)=0.0102
第 10 次迭代: x=0.06047, f(x)=0.00366" />
  <meta name="author" content="Liam" /><link rel="preload stylesheet" as="style" href="https://chlzhong.org/main.min.css" />

  
  <link rel="preload" as="image" href="https://chlzhong.org/theme.svg" />

  <link rel="preload" as="image" href="https://pic-private.zhihu.com/v2-1c006a7896b6ece145052d56f0e1e29c~resize:1440:q75.png?source=1f5c5e47&amp;expiration=1760783972&amp;auth_key=1760783972-0-0-0bdc35ce27dc3e5273de1cf592256883&amp;protocol=v2&amp;sampling=False&amp;animatedImagePlayCount=1&amp;overTime=60&amp;incremental=False&amp;sceneCode=article_draft_web&amp;animatedImageAutoPlay=False&amp;retryCount=3&amp;precoder=False&amp;draft_token=704569937" />

  <link rel="preload" as="image" href="https://chlzhong.org/github.svg" /><link rel="preload" as="image" href="https://chlzhong.org/mastodon.svg" /><link rel="preload" as="image" href="https://chlzhong.org/rss.svg" />

  <script
    defer
    src="https://chlzhong.org/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script><link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script><script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>


<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="/favicon.svg" />
<link rel="shortcut icon" href="/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<meta name="apple-mobile-web-app-title" content="Chlzhong" />
<link rel="manifest" href="/site.webmanifest" />


<meta property="og:title" content="chlzhong.org">
<meta property="og:description" content="Personal blog by Chlzhong">
<meta property="og:image" content="/favicons/og-image-1200x630.png">
<meta property="og:url" content="https://chlzhong.org">
<meta property="og:type" content="website">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="chlzhong.org">
<meta name="twitter:description" content="Personal blog by Chlzhong">
<meta name="twitter:image" content="/favicons/twitter-card-1200x675.png">
  <meta name="generator" content="Hugo 0.148.2">
  <meta itemprop="name" content="线性回归">
  <meta itemprop="description" content="{线性回归}">
  <meta itemprop="datePublished" content="2024-12-05T12:32:24+08:00">
  <meta itemprop="dateModified" content="2024-12-05T12:32:24+08:00">
  <meta itemprop="wordCount" content="483"><meta property="og:url" content="https://chlzhong.org/post/linear-regression/">
  <meta property="og:site_name" content="Chlzhong">
  <meta property="og:title" content="线性回归">
  <meta property="og:description" content="{线性回归}">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-12-05T12:32:24+08:00">
    <meta property="article:modified_time" content="2024-12-05T12:32:24+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="线性回归">
  <meta name="twitter:description" content="{线性回归}">

  <link rel="canonical" href="https://chlzhong.org/post/linear-regression/" />
</head>
<body
    class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"
  ><header
  class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"
>
  <div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto">
    <a
      class="-translate-y-[1px] text-2xl font-medium"
      href="https://chlzhong.org/"
      >Chlzhong</a
    >
    <div
      class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8"
    role="button"
    aria-label="Menu"
  ></div>

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"
  ><nav
      class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"
    ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/archives/"
        >Archives</a
      ><a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/contact/"
        >Contact</a
      ></nav><nav
      class="mt-12 flex justify-center space-x-10 lg:mt-0 lg:items-center ltr:lg:ml-14 rtl:space-x-reverse rtl:lg:mr-14 dark:invert"
    >
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/Liam-Zhong"
        target="_blank"
        rel="me"
      >github</a>
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./mastodon.svg)"
        href="https://mastodon.social/@tuffy1883"
        target="_blank"
        rel="me"
      >mastodon</a>
      <a
        class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href="https://chlzhong.org/index.xml"
        target="_blank"
        rel="alternate"
      >rss</a>
    </nav>
  </div>
</header>
<main
      class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"
    ><article class="post special-page">
  <header class="mb-14">
    <h1 class="my-0! pb-2.5">线性回归</h1><div class="text-xs antialiased opacity-60"><time>Dec 5, 2024</time><span class="mx-1">&middot;</span>
      <span>Liam</span></div></header>

  <section><p>线性回归理解起来很简单，当参数样本只有一个特征与一个标签时就是高中学最小二乘法的那个模型，两个特征及以上时则是大学概率论上学的参数估计，只不过估计参数时选用了梯度下降法。</p>
<p>值得注意的是损失函数的选择，为什么是<strong>平方误差函数</strong>而不是差的绝对值呢？这里其实是因为我们假定偏差的分布符合高斯分布，其对应的概率密度函数就是平方和形式，而且<strong>差的绝对值</strong>（拉普拉斯分布）所对应的极大似然函数没法方便寻优。</p>
<p>而说到梯度下降，在工程实现方面其实也不难，具体见后文代码，主要谈谈背后的数学。</p>
<h3 id="0梯度是什么">0.梯度是什么</h3>
<p>对于一个多元函数，梯度是该函数所有偏导数的向量，指向函数增长最快的方向，大小表示增长的速率。</p>
<p>假设有一个函数 $f(x_1,x_2,…,x_n)$，梯度可以表示为一个向量：</p>
<p>$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$</p>
<p>这里，$\frac{\partial f}{\partial x_i}$ 是函数 $f$ 对每个变量 $x_i$ 的偏导数。</p>
<h3 id="1-梯度下降与泰勒展开的关系">1. <strong>梯度下降与泰勒展开的关系</strong></h3>
<p>先解释下为什么要有梯度下降法：其实最简单的二维凸函数是抛物线 $f(x)=x^2$，很容易通过解方程 $f&rsquo;(x)=0$ 求出最小值在 $x=0$ 处；只是有一些凸函数这样解方程太麻烦，便用梯度下降法来找最值。</p>
<p>最简单情况 ($f(x)=x^2$) 下，若将给定点 $x_0$ 加上 -$\eta\nabla f(x_0)$，就相当于一个逐渐靠近最低点的物理过程。比如取 $x_0$ = 10，$\eta = 0.2$ ，迭代 10 次左右就是差不多靠近了最低点 $x=0$</p>
<blockquote>
<p>初始值 x=10, f(x)=100<br>
第 1 次迭代: x=6.0, f(x)=36.0<br>
第 2 次迭代: x=3.6, f(x)=12.96<br>
第 3 次迭代: x=2.16, f(x)=4.67<br>
第 4 次迭代: x=1.296, f(x)=1.68<br>
第 5 次迭代: x=0.7776, f(x)=0.60<br>
第 6 次迭代: x=0.46656, f(x)=0.22<br>
第 7 次迭代: x=0.27994, f(x)=0.078<br>
第 8 次迭代: x=0.16796, f(x)=0.028<br>
第 9 次迭代: x=0.10078, f(x)=0.0102<br>
第 10 次迭代: x=0.06047, f(x)=0.00366<br></p></blockquote>
<p>不难理解这个 $\eta$ （取个名字叫学习率）取得太小则迭代次数过多，太大则会越过最低点不断震荡。</p>
<p>在实际中的函数没有这么简单，更复杂的算式里梯度下降沿着函数的梯度方向进行优化，而梯度本身就是函数在当前位置的泰勒展开的一阶导数。此时梯度下降法可以看作是在每一步使用泰勒展开的一阶近似来更新参数。具体来说，梯度下降可以看作是泰勒展开的一阶近似的迭代应用。</p>
<h3 id="2-梯度下降的解析解">2. <strong>梯度下降的解析解</strong></h3>
<p>梯度下降法<strong>没有解析解</strong>，因为它是一种数值优化方法。（解析解和数值解的区别不知道的话何承春会头疼呢）</p>
<p>返回的结果就是使目标函数取得最小值的<strong>参数</strong>。</p>
<p>接下来通过一个具体的例子来解释如何理解<strong>批量梯度下降</strong>、<strong>小批量梯度下降</strong>和<strong>随机梯度下降</strong>。</p>
<p>假设我们有一个三元函数（即目标函数是有三个参数的函数），并且我们有100个样本数据。我们希望通过这些数据来优化函数的参数。</p>
<h5 id="1-全批量梯度下降batch-gradient-descent">1. 全批量梯度下降（Batch Gradient Descent）</h5>
<p>全批量梯度下降会在每次迭代中使用<strong>所有 100 个样本</strong>来计算梯度，并更新参数。</p>
<blockquote>
<ol>
<li>从初始参数 $\theta_0$ 开始。<br></li>
<li>计算所有 100 个样本点的梯度，得到梯度平均值：<br>
$\nabla f(\theta) = \frac{1}{100} \sum_{i=1}^{100} \nabla f_i(\theta)$<br></li>
<li>更新参数：<br>
$\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k)$</li>
</ol></blockquote>
<h5 id="2-随机梯度下降stochastic-gradient-descent-sgd">2. 随机梯度下降（Stochastic Gradient Descent, SGD）</h5>
<p>与全批量梯度下降不同，随机梯度下降每次迭代仅使用<strong>一个样本</strong>来计算梯度并更新参数。对于 100 个样本，随机梯度下降每次只随机选择一个样本进行参数更新。</p>
<blockquote>
<ol>
<li>从初始参数 $\theta_0$  开始。<br></li>
<li>随机选择一个样本 $i$，计算该样本的梯度 $\nabla f_i(\theta)$。<br></li>
<li>使用该样本的梯度更新参数：<br>
$\theta_{k+1} = \theta_k - \eta \nabla f_i(\theta_k)$</li>
<li>重复这个过程，直到达到停止条件</li>
</ol></blockquote>
<h5 id="3-小批量梯度下降mini-batch-gradient-descent">3. 小批量梯度下降（Mini-batch Gradient Descent）</h5>
<p>小批量梯度下降是全批量梯度下降和随机梯度下降的折中方法。每次迭代时，它会从所有 100 个样本中随机选取一个<strong>小批量</strong>（例如，10 个样本），用这些样本来计算梯度并更新参数。</p>
<blockquote>
<ol>
<li>从初始参数 $\theta_0$ 开始。<br></li>
<li>随机选择一个小批量样本，假设这个小批量包含 10 个样本 $i_1, i_2, \dots, i_{10}$。<br></li>
<li>计算这 10 个样本的梯度平均值：<br>
$\nabla f(\theta) = \frac{1}{10} \sum_{i=1}^{10} \nabla f_{i}(\theta)$</li>
<li>使用这个平均梯度更新参数：<br>
$\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k)θk+1=θk−η∇f(θk)$</li>
<li>重复这个过程，直到达到停止条件。</li>
</ol></blockquote>
<p>代码实现：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成随机权重</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_w_b</span>():
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.01</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> w, b
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成训练数据</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">synthetic_data</span>(w, b, num_examples):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;生成y = Xw + b + 噪声&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, (num_examples, len(w)))
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(X, w) <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.01</span>, y<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> X, y<span style="color:#f92672">.</span>reshape((<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成迭代器</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">data_iter</span>(batch_size, features, labels):
</span></span><span style="display:flex;"><span>    num_examples <span style="color:#f92672">=</span> len(features)
</span></span><span style="display:flex;"><span>    indices <span style="color:#f92672">=</span> list(range(num_examples))
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 这些样本是随机读取的，没有特定的顺序 </span>
</span></span><span style="display:flex;"><span>    random<span style="color:#f92672">.</span>shuffle(indices)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, num_examples, batch_size):
</span></span><span style="display:flex;"><span>        batch_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(indices[i:min(i <span style="color:#f92672">+</span> batch_size, num_examples)])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">yield</span> features[batch_indices], labels[batch_indices]
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%% md</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 自己完成</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 线性回归模型</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Linear_regression</span>(X, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> X<span style="color:#f92672">*</span>w <span style="color:#f92672">+</span> b <span style="color:#75715e">#return torch.matmul(X, w) + b更好</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 均方损失函数 MSE</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">squared_loss</span>(y_hat, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (y_hat <span style="color:#f92672">-</span> y) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 优化器实现</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sgd</span>(params, lr, batch_size):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;小批量随机梯度下降&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> params:
</span></span><span style="display:flex;"><span>            param <span style="color:#f92672">-=</span> lr <span style="color:#f92672">*</span> param<span style="color:#f92672">.</span>grad <span style="color:#f92672">/</span> batch_size
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%% md</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 测试</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.03</span>
</span></span><span style="display:flex;"><span>num_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>net <span style="color:#f92672">=</span> Linear_regression
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> squared_loss
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># y = -5 * x + 0.1 </span>
</span></span><span style="display:flex;"><span>true_w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#f92672">-</span><span style="color:#ae81ff">5.0</span>])
</span></span><span style="display:flex;"><span>true_b <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>features, labels <span style="color:#f92672">=</span> synthetic_data(true_w, true_b, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>w, b <span style="color:#f92672">=</span> get_w_b()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> data_iter(batch_size, features, labels):
</span></span><span style="display:flex;"><span>        l <span style="color:#f92672">=</span> loss(net(X, w, b), y)
</span></span><span style="display:flex;"><span>        l<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        sgd([w, b], lr, batch_size)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        train_l <span style="color:#f92672">=</span> loss(net(features, w, b), labels)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;epoch </span><span style="color:#e6db74">{</span>epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, loss </span><span style="color:#e6db74">{</span>float(train_l<span style="color:#f92672">.</span>mean())<span style="color:#e6db74">:</span><span style="color:#e6db74">f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;w </span><span style="color:#e6db74">{</span>w<span style="color:#e6db74">}</span><span style="color:#e6db74">, b </span><span style="color:#e6db74">{</span>b<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%% md</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 绘制图形</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> features
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> labels
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x, y, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Samples&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy(), w<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy() <span style="color:#f92672">*</span> x<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy() <span style="color:#f92672">+</span> b<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy(), c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;True function&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, <span style="color:#f92672">-</span><span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.1</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Trained model&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;CQUPT2022212062&#34;</span>, loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;权重&#34;</span>,w)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#%%</span>
</span></span></code></pre></div></section>

  <nav
    class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg leading-[1.2]! *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
  ><a class="ltr:pr-3 rtl:pl-3 no-special-link" href="https://chlzhong.org/post/svm/"
      ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>SVM 支持向量机</span></a
    ><a
      class="justify-end pl-3 ltr:ml-auto rtl:mr-auto no-special-link"
      href="https://chlzhong.org/post/navie-bayes-classifer/"
      ><span>朴素贝叶斯算法与垃圾短信识别</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
    ></nav></article></main><footer
  class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"
>
  <div class="mr-auto">© 2024-2025, liam.so</div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>
</body>
</html>
